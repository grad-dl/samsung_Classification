{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext ipython_unittest\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's Dataset: MNIST (Digit classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/mnist.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Explore MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of train images: 60000\n",
      "Total # of test images: 10000\n",
      "Image Shape: torch.Size([1, 28, 28])\n",
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAcjklEQVR4nO3de7BlZXkn4N8rnYIJKLQaY5lMCjFykSQ4QJBAgSCFYqViMAJj5SJlacpkcBCDllOJxkYziX9MogQymsREqqBqOhYmpBgJMgUoGMitiTJWEETuFVFbhkuLEJv+5o+9Oum05/Rlr929z/n281TtWmevtd79vSyX/Ttrn3Wp1loAgH48a94NAACzJdwBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDNr5t3AnlBV9yZ5TpL75twKAEzr4CSPt9ZevLuFXYZ7JsH+3OEFAAul16/l75t3AwAwA/dNUzTXcK+qH66qP62qf66qp6vqvqr6SFWtnWdfALCaze1r+ap6SZJbkrwgyV8m+XKS45K8I8kZVXVia+1b8+oPAFareR65/89Mgv381tqZrbX/1lp7VZIPJzksyX+fY28AsGpVa23vDzo5ar87k78lvKS1tmWbZc9O8rUkleQFrbVvT/H5G5IcPZtuAWBubmutHbO7RfP6Wv7UYXrdtsGeJK21J6rqr5O8OsnxSa5f7kOGEF/K4TPpEgBWoXl9LX/YML1rmeVfGaaH7oVeAKAr8zpyP3CYPrbM8q3zD9rRhyz3VYWv5QFYZL1e5w4AC2te4b71yPzAZZZvnf/oXugFALoyr3C/c5gu9zf1lw7T5f4mDwAsY17hfuMwfXVV/bsehkvhTkzyZJK/2duNAcBqN5dwb619Ncl1mTzx5rztFl+UZP8kl09zjTsALLp5PhXuv2Ry+9nfr6rTktyR5BWZXAN/V5LfmGNvALBqze1s+eHo/dgkl2US6hcmeUmSi5Mc777yADCduT7PvbX2YJI3z7MHAOiN69wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDNr5t0AML199tlnVP3atWtn1Mnet27duqlrDzjggFFjv+xlL5u69qyzzho19hVXXDF17UknnTRq7M2bN09d+0d/9Eejxj7vvPNG1S+auR25V9V9VdWWeT08r74AYLWb95H7Y0k+ssT8TXu7EQDoxbzD/dHW2ro59wAAXXFCHQB0Zt5H7vtW1S8m+ZEk305ye5KbWmvPzLctAFi95h3uL0xy+Xbz7q2qN7fWPrez4qrasMyiw0d3BgCr1Dy/lv9EktMyCfj9k/x4kj9McnCSv6qqo+bXGgCsXnM7cm+tXbTdrC8l+ZWq2pTkwiTrkrx+J59xzFLzhyP6o2fQJgCsOivxhLqPDdOT59oFAKxSKzHcvzlM959rFwCwSq3EcD9+mN4z1y4AYJWaS7hX1RFV9T1H5lV1cJJLh7fT30AZABbYvE6o+89JLqyqm5Lcn+SJJC9J8tNJ9ktyTZL/MafeAGBVm1e435jksCT/KcmJmfx9/dEkn8/kuvfLW2ttTr0BwKo2l3AfblCz05vUwK465JBDRtXvt99+U9e+5jWvGTX26aefPnXtQQcdNGrs448/fucr8T0ef/zxqWs/+clPjhr7uOOOm7r26aefHjX2gw8+OHXt9ddfP2psds9KPKEOABhBuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHSmWmvz7mHmqmpDkqPn3Qe756STTpq69rrrrhs19r777juqntVl7L97F1544dS1mzZtGjX2GGOex54kDz/88NS1X/ziF0eNvcBua60ds7tFjtwBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA645GvrBjPf/7zp6698847R429du3aUfWL6N577x1V/8QTT4yqP/LII6eufeaZZ0aNvd9++42qh93gka8AgHAHgO4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDozJp5NwBbbdy4cerad7/73aPGPuecc6auvfXWW0eN/f73v39U/RgPPfTQ1LVHHXXUqLE3bdo0qv7YY4+duvYDH/jAqLFhpXPkDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0Jlqrc27h5mrqg1Jjp53H6weBx100NS1jz322KixP/3pT09de8YZZ4wa+x3veMfUtZdccsmosYFdcltr7ZjdLXLkDgCdmUm4V9VZVXVJVd1cVY9XVauqK3ZSc0JVXVNVj1TVd6rq9qq6oKr2mUVPALCo1szoc96b5Kgkm5I8lOTwHa1cVT+b5FNJnkryZ0keSfIzST6c5MQkZ8+oLwBYOLP6Wv6dSQ5N8pwkv7qjFavqOUn+OMkzSU5prb2ltfbuJC9PcmuSs6rqjTPqCwAWzkzCvbV2Y2vtK23Xzs47K8kPJFnfWvuHbT7jqUy+AUh28gsCALC8eZxQ96pheu0Sy25K8mSSE6pq373XEgD0Y1Z/c98dhw3Tu7Zf0FrbXFX3JjkyySFJ7tjRBw2XvC1lh3/zB4CezePI/cBhutzFwVvnT3/hMQAssHkcuc/Mchf2u4kNAItsHkfuW4/MD1xm+db5j+6FXgCgO/MI9zuH6aHbL6iqNUlenGRzknv2ZlMA0It5hPsNw3Spm2KfnOT7k9zSWnt677UEAP2YR7hfmWRjkjdW1bFbZ1bVfkl+a3j70Tn0BQBdmMkJdVV1ZpIzh7cvHKY/VVWXDT9vbK29K0laa49X1S9nEvKfrar1mdx+9nWZXCZ3ZSa3pAUApjCrs+VfnuTc7eYdMryS5P4k79q6oLV2VVW9MslvJHlDkv2S3J3k15L8/i7e6Q4AWILnucOcXXHFDh+guEM///M/P2rsO++8c+crLePII48cNfaWLVtG1cOC8Dx3AEC4A0B3hDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnPPIV5uyAAw6Yuvbv//7vR4192GGHTV079nGz69evH1UPC8IjXwEA4Q4A3RHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZz3OHVeyII44YVf+P//iPU9c+9dRTo8besGHDqPqbb7556tqLLrpo1Ng9/rvJiuV57gCAcAeA7gh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzqyZdwPA9O64445R9eedd97UtZdeeumosU899dS51R9wwAGjxr744ounrn3wwQdHjQ27wpE7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHSmWmvz7mHmqmpDkqPn3Qf07BWveMWo+j/5kz8ZVf+yl71sVP0YV1999dS1559//qix77///lH1rDq3tdaO2d0iR+4A0JmZhHtVnVVVl1TVzVX1eFW1qrpimXUPHpYv91o/i54AYFGtmdHnvDfJUUk2JXkoyeG7UPPFJFctMf9LM+oJABbSrML9nZmE+t1JXpnkxl2o+UJrbd2MxgcABjMJ99bav4Z5Vc3iIwGAKc3qyH0aL6qqtyV5XpJvJbm1tXb77nzAcFb8UnblzwIA0KV5hvvpw+tfVdVnk5zbWntgLh0BQAfmEe5PJvlgJifT3TPM+4kk65KcmuT6qnp5a+3bO/ug5a79c507AItsr1/n3lr7RmvtN1trt7XWHh1eNyV5dZK/TfKjSd66t/sCgF6smJvYtNY2J/n48PbkefYCAKvZign3wTeH6f5z7QIAVrGVFu7HD9N7drgWALCsvR7uVXV0VX3PuFV1WiY3w0mSJW9dCwDs3EzOlq+qM5OcObx94TD9qaq6bPh5Y2vtXcPPv5fkpVV1SyZ3tUsmZ8u/avj5fa21W2bRFwAsolldCvfyJOduN++Q4ZUk9yfZGu6XJ3l9kp9M8tok35fk60k+meTS1trNM+oJABaS57kDc/Hc5z53VP2b3vSmqWt/93d/d9TYY26zfccdd4wa+8gjjxxVz6rjee4AgHAHgO4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM545CuwcDZv3jyq/lnPmv64aMuWLaPGPuecc6au/fM///NRYzMXHvkKAAh3AOiOcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzqyZdwPA6nT88cePqn/zm988t/HHPI99rIcffnhU/VVXXTWjTuiZI3cA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOeOQrrGJHHXXUqPp169ZNXXvaaaeNGvuAAw4YVT9PW7Zsmbp248aNcxubxeHIHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA643nuMNIP/dAPjap/+9vfPnXt2972tlFjH3TQQaPqV6sHHnhgVP26deumrr3ssstGjQ27YvSRe1U9r6reWlV/UVV3V9V3quqxqvp8Vb2lqpYco6pOqKprquqRoeb2qrqgqvYZ2xMALLJZHLmfneSjSb6W5MYkDyT5wSQ/l+TjSV5bVWe31trWgqr62SSfSvJUkj9L8kiSn0ny4SQnDp8JAExhFuF+V5LXJfl0a23L1plV9etJ/i7JGzIJ+k8N85+T5I+TPJPklNbaPwzz35fkhiRnVdUbW2vrZ9AbACyc0V/Lt9ZuaK1dvW2wD/MfTvKx4e0p2yw6K8kPJFm/NdiH9Z9K8t7h7a+O7QsAFtWePlv+u8N08zbzXjVMr11i/ZuSPJnkhKrad082BgC92mNny1fVmiRvGt5uG+SHDdO7tq9prW2uqnuTHJnkkCR37GSMDcssOnz3ugWAfuzJI/cPJfmxJNe01j6zzfwDh+ljy9Rtnb+Y1+gAwEh75Mi9qs5PcmGSLyf5pT0xRpK01o5ZZvwNSY7eU+MCwEo28yP3qnp7kouT/FOSU1trj2y3ytYj8wOztK3zH511bwCwCGYa7lV1QZJLknwpk2B/eInV7hymhy5RvybJizM5Ae+eWfYGAItiZuFeVe/J5CY0X8gk2L+xzKo3DNMzllh2cpLvT3JLa+3pWfUGAItkJuE+3IDmQ0k2JDmttbZxB6tfmWRjkjdW1bHbfMZ+SX5rePvRWfQFAIto9Al1VXVukg9kcse5m5OcX1Xbr3Zfa+2yJGmtPV5Vv5xJyH+2qtZncvvZ12VymdyVmdySFgCYwizOln/xMN0nyQXLrPO5JJdtfdNau6qqXpnkNzK5Pe1+Se5O8mtJfn/b+9ADALunesxRl8Itnhe96EWj6k844YSpay+99NJRY7/gBS8YVb9a3XvvvaPqf/u3f3vq2k984hOjxt6yZcvOV4LZuG25y753ZE/ffhYA2MuEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGfWzLsB+vH85z9/VP3VV189de2hhx46auy1a9eOql+tvvrVr05d+zu/8zujxl6/fv2o+ieffHJUPfTMkTsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnPPK1M6effvqo+g9+8INT1x5xxBGjxn72s589qn61+u53vzt17eWXXz5q7AsuuGDq2k2bNo0aG9hzHLkDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGc8z70zv/ALvzCq/rjjjptRJ3vX17/+9VH111577dS1mzdvHjX2e97znqlrH3nkkVFjA31y5A4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANCZaq3Nu4eZq6oNSY6edx8AMNJtrbVjdrfIkTsAdGZ0uFfV86rqrVX1F1V1d1V9p6oeq6rPV9VbqupZ261/cFW1HbzWj+0JABbZmhl8xtlJPprka0luTPJAkh9M8nNJPp7ktVV1dvve7/+/mOSqJT7vSzPoCQAW1izC/a4kr0vy6dbalq0zq+rXk/xdkjdkEvSf2q7uC621dTMYHwDYxuiv5VtrN7TWrt422If5Dyf52PD2lLHjAAC7ZhZH7jvy3WG6eYllL6qqtyV5XpJvJbm1tXb7Hu4HALq3x8K9qtYkedPw9tolVjl9eG1b89kk57bWHtjFMTYss+jwXWwTALqzJy+F+1CSH0tyTWvtM9vMfzLJB5Mck2Tt8HplJifjnZLk+qrafw/2BQBd2yM3samq85NcnOTLSU5srT2yCzVrknw+ySuSXNBau3jE+G5iA0APVsZNbKrq7ZkE+z8lOXVXgj1JWmubM7l0LklOnnVfALAoZhruVXVBkksyuVb91OGM+d3xzWHqa3kAmNLMwr2q3pPkw0m+kEmwf2OKjzl+mN4zq74AYNHMJNyr6n2ZnEC3IclprbWNO1j36O1vSTvMPy3JO4e3V8yiLwBYRKMvhauqc5N8IMkzSW5Ocn5Vbb/afa21y4affy/JS6vqliQPDfN+Ismrhp/f11q7ZWxfALCoZnGd+4uH6T5JLlhmnc8luWz4+fIkr0/yk0lem+T7knw9ySeTXNpau3kGPQHAwvI8dwBYuVbGpXAAwHwJdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM70Gu4Hz7sBAJiBg6cpWjPjJlaKx4fpfcssP3yYfnnPt9IN22w6ttt0bLfdZ5tNZyVvt4Pzb3m2W6q1NttWVoGq2pAkrbVj5t3LamGbTcd2m47ttvtss+n0ut16/VoeABaWcAeAzgh3AOiMcAeAzgh3AOjMQp4tDwA9c+QOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ1ZqHCvqh+uqj+tqn+uqqer6r6q+khVrZ13byvVsI3aMq+H593fvFTVWVV1SVXdXFWPD9vjip3UnFBV11TVI1X1naq6vaouqKp99lbf87Y7262qDt7Bvteqav3e7n8equp5VfXWqvqLqrp72Hceq6rPV9VbqmrJf8cXfX/b3e3W2/7W6/Pcv0dVvSTJLUlekOQvM3l273FJ3pHkjKo6sbX2rTm2uJI9luQjS8zftLcbWUHem+SoTLbBQ/m3Z0Ivqap+NsmnkjyV5M+SPJLkZ5J8OMmJSc7ek82uILu13QZfTHLVEvO/NMO+VrKzk3w0ydeS3JjkgSQ/mOTnknw8yWur6uy2zR3J7G9Jpthugz72t9baQrySfCZJS/Jft5v/e8P8j827x5X4SnJfkvvm3cdKeyU5NclLk1SSU4Z96Ipl1n1Okm8keTrJsdvM3y+TXzhbkjfO+79pBW63g4fll8277zlvs1dlEszP2m7+CzMJrJbkDdvMt79Nt9262t8W4mv54aj91ZkE1R9st/j9Sb6d5Jeqav+93BqrVGvtxtbaV9rwr8JOnJXkB5Ksb639wzaf8VQmR7JJ8qt7oM0VZze3G0laaze01q5urW3Zbv7DST42vD1lm0X2t0y13bqyKF/LnzpMr1vif+gnquqvMwn/45Ncv7ebWwX2rapfTPIjmfwidHuSm1prz8y3rVXjVcP02iWW3ZTkySQnVNW+rbWn915bq8aLquptSZ6X5FtJbm2t3T7nnlaK7w7TzdvMs7/t3FLbbasu9rdFCffDhuldyyz/SibhfmiE+1JemOTy7ebdW1Vvbq19bh4NrTLL7n+ttc1VdW+SI5MckuSOvdnYKnH68PpXVfXZJOe21h6YS0crQFWtSfKm4e22QW5/24EdbLetutjfFuJr+SQHDtPHllm+df5Be6GX1eYTSU7LJOD3T/LjSf4wk79P/VVVHTW/1lYN+990nkzywSTHJFk7vF6ZyclRpyS5fsH/lPahJD+W5JrW2me2mW9/27HltltX+9uihDtTaq1dNPzt6uuttSdba19qrf1KJici/ock6+bbIb1qrX2jtfabrbXbWmuPDq+bMvmW7W+T/GiSt863y/moqvOTXJjJVT+/NOd2Vo0dbbfe9rdFCfetv6keuMzyrfMf3Qu99GLrCSknz7WL1cH+N0Ottc2ZXMqULOD+V1VvT3Jxkn9Kcmpr7ZHtVrG/LWEXttuSVuv+tijhfucwPXSZ5S8dpsv9TZ7v9c1humq+ppqjZfe/4e9/L87kxJ579mZTq9xC7n9VdUGSSzK55vrU4czv7dnftrOL221HVt3+tijhfuMwffUSdyV6diY3dXgyyd/s7cZWseOH6cL8AzHCDcP0jCWWnZzk+5PcssBnLk9j4fa/qnpPJjeh+UImAfWNZVa1v21jN7bbjqy6/W0hwr219tUk12VyEth52y2+KJPfxi5vrX17L7e2olXVEUudQFJVBye5dHi7w1uukiS5MsnGJG+sqmO3zqyq/ZL81vD2o/NobCWrqqOXurVqVZ2W5J3D24XY/6rqfZmcCLYhyWmttY07WN3+Ntid7dbb/laLci+JJW4/e0eSV2RyDfxdSU5obj/771TVukxOPrkpyf1JnkjykiQ/ncndrq5J8vrW2r/Mq8d5qaozk5w5vH1hktdk8lv9zcO8ja21d223/pWZ3A50fSa3A31dJpctXZnknEW4scvubLfh8qOXZvL/24eG5T+Rf7uO+32tta1h1a2qOjfJZUmeyeSr5aXOgr+vtXbZNjULv7/t7nbrbn+b9y3y9uYryX/M5NKuryX5l0wC6yNJ1s67t5X4yuQykP+VyZmlj2Zy44dvJvk/mVwnWvPucY7bZl0mt6pc7nXfEjUnZvIL0f9L8p0k/zeTI4J95v3fsxK3W5K3JPnfmdxZclMmt1N9IJN7pZ807/+WFbTNWpLP2t/Gbbfe9reFOXIHgEWxEH9zB4BFItwBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA68/8B3yPoSco2MUQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = trainset[0]\n",
    "print(\"Total # of train images: {}\".format(len(trainset)))\n",
    "print(\"Total # of test images: {}\".format(len(valset)))\n",
    "print(\"Image Shape: {}\".format(image.size()))\n",
    "print(\"Label: {}\".format(label))\n",
    "plt.imshow(image.numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement CustomLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "def create_random_targets(*shape):\n",
    "    batch_size, num_class, H, W = shape\n",
    "    eye = torch.eye(num_class)\n",
    "    return eye[np.random.choice(num_class, batch_size * H * W)].view(batch_size, num_class, H, W)\n",
    "    \n",
    "def is_almost_equal(tens_a, tens_b, delta=1e-5):\n",
    "    return torch.all(torch.lt(torch.abs(torch.add(tens_a, -tens_b)), delta))\n",
    "\n",
    "class TestCustomLinearLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.in_features = 5\n",
    "        self.out_features = 10\n",
    "        \n",
    "    def test_layer_output_size(self):\n",
    "        inputs = torch.randn((self.batch_size, self.in_features))\n",
    "        fc = CustomLinear(self.in_features, self.out_features)\n",
    "        \n",
    "        outputs = fc(inputs)\n",
    "        self.assertTupleEqual((self.batch_size, self.out_features), outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Linear Classifer using the CustomLinear\n",
    "\n",
    "![](imgs/network_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_class=10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = CustomLinear(input_size, hidden_size)\n",
    "        self.fc2 = CustomLinear(hidden_size, num_class)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        flatten_inputs = ?\n",
    "        o1 = ?\n",
    "        logits = ?\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.005s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "class TestCustomLinearClassifier(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.num_class = 10\n",
    "        self.channel, self.height, self.width = 1, 28, 28\n",
    "        \n",
    "    def test_model_output_size(self):\n",
    "        images = torch.randn(self.batch_size, self.channel, self.height, self.width)\n",
    "        model = CustomLinearClassifier()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        self.assertTupleEqual((self.batch_size, self.num_class), outputs.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Don't need to wait. Let's Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "from metric import AverageMeterSet\n",
    "\n",
    "\n",
    "def print_metrics(phase, average_meter_set: AverageMeterSet):\n",
    "    results = [\"{}: {:4f}\".format(k, v) for k, v in average_meter_set.averages().items()]\n",
    "    print(\"{}: {}\".format(phase, \", \".join(results)))\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    average_meter_set = AverageMeterSet()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        average_meter_set.update('loss', loss.item())\n",
    "\n",
    "    return average_meter_set, model\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    average_meter_set = AverageMeterSet()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, preds = logits.max(1)\n",
    "            average_meter_set.update('correct', preds.eq(labels).sum().item(), n=inputs.size(0))\n",
    "            average_meter_set.update('loss', loss.item())\n",
    "\n",
    "    return average_meter_set\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, device, num_epochs=25):\n",
    "    model = model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                average_meter_set, model = train_one_epoch(model, dataloaders[phase], optimizer, device)\n",
    "                print_metrics(phase, average_meter_set)\n",
    "                # lr_scheduler.step()\n",
    "            else:\n",
    "                average_meter_set = validate(model, dataloaders[phase], device)\n",
    "                print_metrics(phase, average_meter_set)\n",
    "                epoch_acc = average_meter_set.averages()['correct']\n",
    "\n",
    "                if epoch_acc > best_acc:\n",
    "                    print(\"Saving the Best Model\")\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val Accuracy: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': trainloader,\n",
    "    'val': valloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Our Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model and a Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 74.503100\n",
      "val: correct: 0.828500, loss: 22.516309\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 20.336023\n",
      "val: correct: 0.853700, loss: 16.500135\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 15.748947\n",
      "val: correct: 0.865000, loss: 13.845797\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 13.285053\n",
      "val: correct: 0.882200, loss: 11.228973\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 11.801427\n",
      "val: correct: 0.869800, loss: 11.288218\n",
      "0m 11s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 10.561025\n",
      "val: correct: 0.884500, loss: 9.482217\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 9.580128\n",
      "val: correct: 0.877800, loss: 9.313546\n",
      "0m 11s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 8.778110\n",
      "val: correct: 0.887400, loss: 8.527984\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 8.266684\n",
      "val: correct: 0.894400, loss: 7.794343\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 7.824831\n",
      "val: correct: 0.885300, loss: 7.699096\n",
      "0m 11s\n",
      "Best Val Accuracy: 0.894400\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "custom_linear_model = CustomLinearClassifier()\n",
    "optimizer_ft = optim.Adam(custom_linear_model.parameters(), lr=1e-3)\n",
    "custom_linear_model = train_model(custom_linear_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement a Linear Model using PyTorch Modules\n",
    "\n",
    "PyTorch provides a lot of built-in modules such as `CustomLinear`. We'll rebuild our model using `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_class=10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = ?\n",
    "        self.fc2 = ?\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        flatten_inputs = ?\n",
    "        h1 = ?\n",
    "        logits = ?\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.006s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "class TestLinearClassifier(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.num_class = 10\n",
    "        self.channel, self.height, self.width = 1, 28, 28\n",
    "        \n",
    "    def test_model_output_size(self):\n",
    "        images = torch.randn(self.batch_size, self.channel, self.height, self.width)\n",
    "        model = LinearModel()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        self.assertTupleEqual((self.batch_size, self.num_class), outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.418896\n",
      "val: correct: 0.899500, loss: 0.341470\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.333592\n",
      "val: correct: 0.908300, loss: 0.308915\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.314869\n",
      "val: correct: 0.919200, loss: 0.285819\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.312661\n",
      "val: correct: 0.920300, loss: 0.286972\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.307898\n",
      "val: correct: 0.915900, loss: 0.296482\n",
      "0m 11s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.303331\n",
      "val: correct: 0.918600, loss: 0.284076\n",
      "0m 11s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.299332\n",
      "val: correct: 0.913900, loss: 0.300449\n",
      "0m 10s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.295895\n",
      "val: correct: 0.919200, loss: 0.286723\n",
      "0m 10s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.295692\n",
      "val: correct: 0.911000, loss: 0.301992\n",
      "0m 10s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.291960\n",
      "val: correct: 0.918600, loss: 0.284459\n",
      "0m 11s\n",
      "Best Val Accuracy: 0.920300\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "linear_model = LinearModel()\n",
    "optimizer_ft = optim.Adam(linear_model.parameters(), lr=1e-3)\n",
    "linear_model = train_model(linear_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Non-linearity\n",
    "\n",
    "As we learned before, non-linearity helps models represent more complex features. We will add `nn.ReLU()` which is one of activation functions for the non-linearity.\n",
    "![image.png](imgs/activation.png)\n",
    "\n",
    "### Exercise 4: Insert a ReLU after fc1 of the Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_class=10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = ?\n",
    "        h1 = ?  # Use ReLU after fc1\n",
    "        logits = ?\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.004s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "class TestNonLinearModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.num_class = 10\n",
    "        self.channel, self.height, self.width = 1, 28, 28\n",
    "        \n",
    "    def test_model_output_size(self):\n",
    "        images = torch.randn(self.batch_size, self.channel, self.height, self.width)  # shape = (batch_size, 1, 28, 28)\n",
    "        model = NonLinearModel()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        self.assertTupleEqual((self.batch_size, self.num_class), outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.399979\n",
      "val: correct: 0.936500, loss: 0.220997\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.198175\n",
      "val: correct: 0.954200, loss: 0.159310\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.135595\n",
      "val: correct: 0.965800, loss: 0.112284\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.103824\n",
      "val: correct: 0.967100, loss: 0.108543\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.086404\n",
      "val: correct: 0.970900, loss: 0.096481\n",
      "Saving the Best Model\n",
      "0m 12s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.074477\n",
      "val: correct: 0.973300, loss: 0.087732\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.063799\n",
      "val: correct: 0.975300, loss: 0.082540\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.058048\n",
      "val: correct: 0.974600, loss: 0.080761\n",
      "0m 12s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.053324\n",
      "val: correct: 0.976000, loss: 0.077385\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.046393\n",
      "val: correct: 0.974800, loss: 0.080411\n",
      "0m 11s\n",
      "Best Val Accuracy: 0.976000\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "non_linear_model = NonLinearModel()\n",
    "optimizer_ft = optim.Adam(non_linear_model.parameters(), lr=1e-3)\n",
    "non_linear_model = train_model(non_linear_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement a Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(320, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h1_1 = ?\n",
    "        pool1 = ?  \n",
    "        h1 = ?  # Use ReLU here\n",
    "        \n",
    "        h2_1 = ?\n",
    "        pool2 = ?\n",
    "        h2 = ?  # Use ReLU here\n",
    "        \n",
    "        h2 = ?  # Reshape features\n",
    "        logits = ?\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.007s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "class TestCNNModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.num_class = 10\n",
    "        self.channel, self.height, self.width = 1, 28, 28\n",
    "        \n",
    "    def test_model_output_size(self):\n",
    "        images = torch.randn(self.batch_size, self.channel, self.height, self.width)\n",
    "        model = CNN()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        self.assertTupleEqual((self.batch_size, self.num_class), outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.304544\n",
      "val: correct: 0.974000, loss: 0.083753\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.079605\n",
      "val: correct: 0.983800, loss: 0.051238\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.059709\n",
      "val: correct: 0.984300, loss: 0.044509\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.049275\n",
      "val: correct: 0.987500, loss: 0.037873\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.042901\n",
      "val: correct: 0.986900, loss: 0.038626\n",
      "0m 11s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.036446\n",
      "val: correct: 0.988200, loss: 0.036163\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.032773\n",
      "val: correct: 0.989400, loss: 0.031272\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.029147\n",
      "val: correct: 0.989100, loss: 0.030303\n",
      "0m 11s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.026610\n",
      "val: correct: 0.990600, loss: 0.029312\n",
      "Saving the Best Model\n",
      "0m 11s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.023692\n",
      "val: correct: 0.988100, loss: 0.037470\n",
      "0m 11s\n",
      "Best Val Accuracy: 0.990600\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "cnn_model = CNN()\n",
    "optimizer_ft = optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "cnn_model = train_model(cnn_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 10, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(10, 20, kernel_size=3, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n",
    "        self.conv1_4 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(20, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(30, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(30, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_4 = nn.Conv2d(30, 20, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(980, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h1_1 = self.relu(self.conv1_1(inputs))\n",
    "        h1_2 = self.relu(self.conv1_2(h1_1))\n",
    "        h1_3 = self.relu(self.conv1_3(h1_2))\n",
    "        h1_4 = self.relu(self.conv1_4(h1_3))\n",
    "        h1 = self.pool1(h1_4)\n",
    "        \n",
    "        h2_1 = self.relu(self.conv2_1(h1))\n",
    "        h2_2 = self.relu(self.conv2_2(h2_1))\n",
    "        h2_3 = self.relu(self.conv2_3(h2_2))\n",
    "        h2_4 = self.relu(self.conv2_4(h2_3))\n",
    "        h2 = self.pool2(h2_4)\n",
    "        \n",
    "        h2 = h2.view(-1, 980)\n",
    "        logits = self.fc(h2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.330007\n",
      "val: correct: 0.975600, loss: 0.071013\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.068499\n",
      "val: correct: 0.986900, loss: 0.040292\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.050391\n",
      "val: correct: 0.988200, loss: 0.035908\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.039647\n",
      "val: correct: 0.987900, loss: 0.038367\n",
      "0m 13s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.032695\n",
      "val: correct: 0.989400, loss: 0.033648\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.026832\n",
      "val: correct: 0.988200, loss: 0.037657\n",
      "0m 13s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.024850\n",
      "val: correct: 0.988300, loss: 0.040695\n",
      "0m 13s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.023824\n",
      "val: correct: 0.988900, loss: 0.035053\n",
      "0m 13s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.019684\n",
      "val: correct: 0.990300, loss: 0.034791\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.019384\n",
      "val: correct: 0.990100, loss: 0.032528\n",
      "0m 13s\n",
      "Best Val Accuracy: 0.990300\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "x_cnn_model = XCNN()\n",
    "optimizer_ft = optim.Adam(x_cnn_model.parameters(), lr=1e-3)\n",
    "x_cnn_model = train_model(x_cnn_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implement a Skip Connection Between `h1` and `h2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RXCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_1 = nn.Conv2d(1, 10, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(10, 20, kernel_size=3, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n",
    "        self.conv1_4 = nn.Conv2d(20, 20, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(20, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(30, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(30, 30, kernel_size=3, padding=1)\n",
    "        self.conv2_4 = nn.Conv2d(30, 20, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(980, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h1_1 = self.relu(self.conv1_1(inputs))\n",
    "        h1_2 = self.relu(self.conv1_2(h1_1))\n",
    "        h1_3 = self.relu(self.conv1_3(h1_2))\n",
    "        h1_4 = self.relu(self.conv1_4(h1_3))\n",
    "        h1 = self.pool1(h1_4)\n",
    "        \n",
    "        h2_1 = self.relu(self.conv2_1(h1))\n",
    "        h2_2 = self.relu(self.conv2_2(h2_1))\n",
    "        h2_3 = self.relu(self.conv2_3(h2_2))\n",
    "        h2_4 = self.relu(self.conv2_4(h2_3))\n",
    "        h2 = self.pool2(h2_4)\n",
    "        \n",
    "        h2 = ?\n",
    "        \n",
    "        h2 = h2.view(-1, 980)\n",
    "        logits = self.fc(h2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".\n----------------------------------------------------------------------\nRan 1 test in 0.011s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.011s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "import numpy as np\n",
    "\n",
    "class TestRXCNNModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 3\n",
    "        self.num_class = 10\n",
    "        self.channel, self.height, self.width = 1, 28, 28\n",
    "        \n",
    "    def test_model_output_size(self):\n",
    "        images = torch.randn(self.batch_size, self.channel, self.height, self.width)\n",
    "        model = RXCNN()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        self.assertTupleEqual((self.batch_size, self.num_class), outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.271691\n",
      "val: correct: 0.981400, loss: 0.055445\n",
      "Saving the Best Model\n",
      "0m 14s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.053741\n",
      "val: correct: 0.988900, loss: 0.033965\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.038109\n",
      "val: correct: 0.986100, loss: 0.041920\n",
      "0m 13s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.032071\n",
      "val: correct: 0.990600, loss: 0.025405\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.027102\n",
      "val: correct: 0.991400, loss: 0.023625\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.023289\n",
      "val: correct: 0.992300, loss: 0.023166\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.020907\n",
      "val: correct: 0.989300, loss: 0.030988\n",
      "0m 13s\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.018826\n",
      "val: correct: 0.991700, loss: 0.027668\n",
      "0m 13s\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.015636\n",
      "val: correct: 0.992200, loss: 0.026062\n",
      "0m 13s\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR 0.001\n",
      "train: loss: 0.015530\n",
      "val: correct: 0.992700, loss: 0.027304\n",
      "Saving the Best Model\n",
      "0m 13s\n",
      "Best Val Accuracy: 0.992700\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "residual_cnn_model = RXCNN()\n",
    "optimizer_ft = optim.Adam(residual_cnn_model.parameters(), lr=1e-3)\n",
    "residual_cnn_model = train_model(residual_cnn_model, dataloaders, optimizer_ft, device, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
